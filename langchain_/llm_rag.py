import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings.base import Embeddings
from typing import List, Dict
import os
import traceback
import hashlib

from langchain.memory import ConversationBufferMemory

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="çŸ¥è¯†åº“èŠå¤©åŠ©æ‰‹", layout="wide")

# è®¾ç½® OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-xxxx"
os.environ["OPENAI_API_BASE"] = "https://newapi.xxx.com/v1"  # æ·»åŠ è‡ªå®šä¹‰ base URL

# æ¨¡å‹é…ç½®
EMBEDDING_MODEL = "shaw/dmeta-embedding-zh"  # ä¾‹å¦‚: "text-embedding-3-small"
CHAT_MODEL = "gemini-exp-1206"  # ä¾‹å¦‚: "gpt-3.5-turbo"

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
def initialize_vectorstore():
    if 'vectorstore' not in st.session_state:
        st.session_state.vectorstore = None

# åˆå§‹åŒ–èŠå¤©å†å²
def initialize_chat_history():
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

# å¤„ç†æ–‡ä»¶ä¸Šä¼ 
def process_file(uploaded_file):
    # è·å–æ–‡ä»¶æ‰©å±•å
    file_extension = uploaded_file.name.split(".")[-1]
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with open(f"temp.{file_extension}", "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        if file_extension == "pdf":
            loader = PyPDFLoader(f"temp.{file_extension}")
        else:
            loader = TextLoader(f"temp.{file_extension}")
        
        # åŠ è½½æ–‡æ¡£
        documents = loader.load()
        
        # ä¿®æ”¹æ–‡æœ¬åˆ†å‰²å™¨çš„å‚æ•°
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=500,  # å‡å°chunkå¤§å°
            chunk_overlap=50,  # å‡å°overlap
            length_function=len,
            is_separator_regex=False,
        )
        texts = text_splitter.split_documents(documents)
        
        # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
        if not texts:
            st.error("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
            return None
            
        # ä½¿ç”¨CustomEmbeddings
        embeddings = CustomEmbeddings(
            model=EMBEDDING_MODEL,
            api_base=os.getenv("OPENAI_API_BASE")
        )
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"å‡†å¤‡å¤„ç†çš„æ–‡æœ¬æ•°é‡: {len(texts)}")
        
        # è¯¦ç»†çš„APIå“åº”è°ƒè¯•
        try:
            test_text = texts[0].page_content[:100]
            print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
            
            # æµ‹è¯•åµŒå…¥
            test_embedding = embeddings.embed_query(test_text)
            print(f"æµ‹è¯•åµŒå…¥å‘é‡ç»´åº¦: {len(test_embedding)}")
            print(f"åµŒå…¥å‘é‡å‰10ä¸ªå€¼: {test_embedding[:10]}")
            
        except Exception as e:
            print(f"æµ‹è¯•åµŒå…¥å¤±è´¥: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e)}")
            raise
        
        # æ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
        max_retries = 3
        for attempt in range(max_retries):
            try:
                vectorstore = FAISS.from_documents(
                    documents=texts,
                    embedding=embeddings
                )
                break
            except Exception as e:
                print(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
                if attempt == max_retries - 1:
                    raise
                continue
        
        return vectorstore
        
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        return None
    finally:
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(f"temp.{file_extension}"):
            os.remove(f"temp.{file_extension}")

class CustomEmbeddings(Embeddings):
    def __init__(self, model: str, api_base: str):
        self.model = model
        self.api_base = api_base
        self.client = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"), base_url=api_base)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        try:
            # ç¡®ä¿æ¯ä¸ªæ–‡æœ¬éƒ½å•ç‹¬å¤„ç†
            embeddings = []
            for text in texts:
                response = self.client.client.create(
                    model=self.model,
                    input=[text]  # æ³¨æ„è¿™é‡Œéœ€è¦ä¼ å…¥åˆ—è¡¨
                )
                embedding = response.data[0].embedding
                if len(embedding) == 0:
                    raise ValueError(f"æ£€æµ‹åˆ°ç©ºçš„åµŒå…¥å‘é‡ï¼Œæ–‡æœ¬: {text[:100]}...")
                embeddings.append(embedding)
            
            print(f"æˆåŠŸç”ŸæˆåµŒå…¥å‘é‡æ•°é‡: {len(embeddings)}")
            return embeddings
            
        except Exception as e:
            print(f"åµŒå…¥æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        try:
            response = self.client.client.create(
                model=self.model,
                input=[text]
            )
            embedding = response.data[0].embedding
            if len(embedding) == 0:
                raise ValueError("æ£€æµ‹åˆ°ç©ºçš„åµŒå…¥å‘é‡")
            return embedding
        except Exception as e:
            print(f"åµŒå…¥æŸ¥è¯¢å¤±è´¥: {str(e)}")
            raise

def get_file_hash(file_content: bytes) -> str:
    """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    return hashlib.md5(file_content).hexdigest()

def load_processed_files() -> Dict[str, bool]:
    """ä»session_stateåŠ è½½å·²å¤„ç†æ–‡ä»¶çš„è®°å½•"""
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = {}
    return st.session_state.processed_files

def mark_file_as_processed(file_hash: str):
    """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
    st.session_state.processed_files[file_hash] = True

def is_file_processed(file_hash: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
    return st.session_state.processed_files.get(file_hash, False)

@st.dialog("å¼•ç”¨å†…å®¹")
def show_sources(sources):
    if not sources:
        st.warning("æ²¡æœ‰æ‰¾åˆ°å¼•ç”¨å†…å®¹")
        return
        
    for i, source in enumerate(sources, 1):
        st.markdown(f"### å¼•ç”¨ {i}")
        st.markdown(f"```\n{source['content']}\n```")
        if source['source'] != 'æœªçŸ¥æ¥æº':
            st.caption(f"æ¥æº: {source['source']}")
        st.markdown("---")

def get_answer(question: str, vectorstore, chat_history=None):
    try:
        llm = ChatOpenAI(
            model=CHAT_MODEL,
            temperature=0.7,
            base_url=os.getenv("OPENAI_API_BASE"),
            streaming=True
        )
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ä¿å­˜
        docs = vectorstore.similarity_search(question, k=3)
        sources = [{"content": doc.page_content, "source": getattr(doc.metadata, 'source', 'æœªçŸ¥æ¥æº')} 
                  for doc in docs]
        
        # æ„å»ºæç¤ºæ¨¡æ¿
        template = """åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»å·²çŸ¥ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚"
        å·²çŸ¥ä¿¡æ¯:
        {context}
        
        èŠå¤©å†å²ï¼š
        {chat_history}
        
        é—®é¢˜: {question}
        å›ç­”: """
        
        # æ ¼å¼åŒ–èŠå¤©å†å²
        chat_history_str = ""
        if chat_history:
            chat_history_str = "\n".join([f"é—®ï¼š{q}\nç­”ï¼š{a}" if a else f"é—®ï¼š{q}" 
                                         for q, a in chat_history])
        
        # æ„å»ºæç¤º
        prompt = template.format(
            context="\n".join([doc.page_content for doc in docs]),
            chat_history=chat_history_str,
            question=question
        )
        with st.chat_message("assistant"):
            # æµå¼è¾“å‡ºå›ç­”
            response_placeholder = st.empty()
            full_response = ""
            
            for chunk in llm.stream(prompt):
                content = chunk.content
                full_response += content
                response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
        
            # æ£€æŸ¥å›ç­”æ˜¯å¦æ¥è‡ªçŸ¥è¯†åº“
            if not "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»å·²çŸ¥ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆ" in full_response:
                # æ·»åŠ å¼•ç”¨æ¥æºé“¾æ¥
                if st.button("ğŸ“š å¼•ç”¨æ¥æº", key=f"source_btn_{hash(question)}", type="secondary", use_container_width=False):
                    show_sources(sources)
        
        return full_response, sources
        
    except Exception as e:
        st.error(f"è·å–å›ç­”æ—¶å‡ºé”™: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚", []

def main():
    st.title("æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    # åˆå§‹åŒ–session state
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = []
    if 'vectorstore' not in st.session_state:
        st.session_state.vectorstore = None
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = {}
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # åˆ›å»ºå·¦å³å¸ƒå±€
    left_col, right_col = st.columns([1, 2])
    
    # å·¦ä¾§ï¼šæ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    with left_col:
        st.header("æ–‡æ¡£ä¸Šä¼ ")
        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", 
                                        type=["txt", "md", "pdf"], 
                                        accept_multiple_files=True)
        
        # æ›´æ–°ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
        if uploaded_files:
            st.session_state.uploaded_files = uploaded_files
        
        # æ˜¾ç¤ºå·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
        if st.session_state.uploaded_files:
            st.write("å·²ä¸Šä¼ æ–‡ä»¶ï¼š")
            for file in st.session_state.uploaded_files:
                file_hash = get_file_hash(file.getvalue())
                status = "å·²å¤„ç†" if st.session_state.processed_files.get(file_hash, False) else "æœªå¤„ç†"
                st.write(f"- {file.name} ({status})")
        
        # æäº¤æŒ‰é’®
        if st.button("å¼€å§‹å¤„ç†æ–‡ä»¶"):
            if not st.session_state.uploaded_files:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
            else:
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                    for file in st.session_state.uploaded_files:
                        file_hash = get_file_hash(file.getvalue())
                        if not st.session_state.processed_files.get(file_hash, False):
                            st.write(f"å¤„ç†æ–‡ä»¶: {file.name}")
                            vectorstore = process_file(file)
                            if vectorstore:
                                if st.session_state.vectorstore is None:
                                    st.session_state.vectorstore = vectorstore
                                else:
                                    st.session_state.vectorstore.merge_from(vectorstore)
                                st.session_state.processed_files[file_hash] = True
                                st.success(f"{file.name} å¤„ç†å®Œæˆ")
                            else:
                                st.error(f"{file.name} å¤„ç†å¤±è´¥")
                        else:
                            st.info(f"è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {file.name}")
    
    # å³ä¾§ï¼šèŠå¤©ç•Œé¢
    with right_col:
        st.header("å¯¹è¯")
        
        with st.container(border=True):
            # æ˜¾ç¤ºèŠå¤©å†å²ï¼ˆæ­£åºæ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯ï¼‰
            if st.session_state.vectorstore is not None:
                for i, message in enumerate(st.session_state.chat_history):
                    with st.chat_message(message["role"]):
                        st.write(message["content"])
                        if (message["role"] == "assistant" and 
                            "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»å·²çŸ¥ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆ" not in message["content"] and
                            message.get("sources")):
                            if st.button("ğŸ“š å¼•ç”¨æ¥æº", key=f"source_btn_{i}", type="secondary", use_container_width=False):
                                show_sources(message["sources"])
            
            question = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
            
            if question:
                # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
                with st.chat_message("user"):
                    st.write(question)
                
                # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²è®°å½•
                st.session_state.chat_history.append({
                    "role": "user",
                    "content": question
                })
                
                with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):                
                    # ç”Ÿæˆå›ç­”
                    response, sources = get_answer(
                        question, 
                        st.session_state.vectorstore,
                        chat_history=[(msg["content"], None) for msg in st.session_state.chat_history[:-1]]
                    )
                    
                    # æ·»åŠ åŠ©æ‰‹å›ç­”åˆ°å†å²è®°å½•
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": response,
                        "sources": sources
                    })
                
                # å¼ºåˆ¶é‡æ–°æ¸²æŸ“
                st.rerun()

if __name__ == "__main__":
    main()
